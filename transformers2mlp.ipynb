{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":47283,"sourceType":"datasetVersion","datasetId":34683}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:22:05.628318Z","iopub.execute_input":"2024-04-04T00:22:05.628784Z","iopub.status.idle":"2024-04-04T00:22:25.018731Z","shell.execute_reply.started":"2024-04-04T00:22:05.628730Z","shell.execute_reply":"2024-04-04T00:22:25.017693Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-lvd51mc6\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-lvd51mc6\n  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy (from clip==1.0)\n  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.1.2+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.16.2+cpu)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.3.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nDownloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=148de58213793cf13a6c07182382281beafd99f694276151c321baba64a107fa\n  Stored in directory: /tmp/pip-ephem-wheel-cache-o507ygtg/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: ftfy, clip\nSuccessfully installed clip-1.0 ftfy-6.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import logging\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom clip import clip\nimport kornia.losses as losses\n\n\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset,DataLoader\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\n\nfrom glob import glob\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:22:25.021331Z","iopub.execute_input":"2024-04-04T00:22:25.022087Z","iopub.status.idle":"2024-04-04T00:22:34.778202Z","shell.execute_reply.started":"2024-04-04T00:22:25.022051Z","shell.execute_reply":"2024-04-04T00:22:34.777237Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, input_size=2, hidden_size=64, num_layers = 5, output_size=3):\n        super(MLP, self).__init__()\n        \n        self.input = nn.Linear(input_size, hidden_size)\n        self.main_activation = nn.ReLU()\n        \n        model = []\n        for i in range(num_layers):\n            model += [nn.Linear(hidden_size, hidden_size),\n                      self.main_activation\n                     ]\n        model += [nn.Linear(hidden_size, output_size),nn.Tanh()]\n        self.model = nn.Sequential(*model)\n\n    def set_parameters(self, estimated_parameters):\n        i = 0\n        for name, param in self.named_parameters():\n            if 'weight' in name:\n                self.state_dict()[name].copy_(estimated_parameters[i])\n                i += 1\n            elif 'bias' in name:\n                self.state_dict()[name].copy_(estimated_parameters[i])\n                i += 1\n                \n    def forward(self, x):\n        x = self.main_activation(self.input(x))\n        x = self.model(x)\n        return x\n    \ndef get_parameter_shapes(model):\n    param_shapes = {}\n    for name, param in model.named_parameters():\n        if 'weight' in name:\n            param_shapes[name[:-7] + ' weights'] = param.shape\n        elif 'bias' in name:\n            param_shapes[name[:-5] + ' biases'] = param.shape\n    return param_shapes","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:22:34.779692Z","iopub.execute_input":"2024-04-04T00:22:34.780178Z","iopub.status.idle":"2024-04-04T00:22:34.798958Z","shell.execute_reply.started":"2024-04-04T00:22:34.780148Z","shell.execute_reply":"2024-04-04T00:22:34.797749Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch.nn.utils.spectral_norm as spectral_norm\n\ndef get_nonspade_norm_layer(opt, norm_type='instance'):\n    # helper function to get # output channels of the previous layer\n    def get_out_channel(layer):\n        if hasattr(layer, 'out_channels'):\n            return getattr(layer, 'out_channels')\n        return layer.weight.size(0)\n\n    # this function will be returned\n    def add_norm_layer(layer):\n        nonlocal norm_type\n        if norm_type.startswith('spectral'):\n            layer = spectral_norm(layer)\n            subnorm_type = norm_type[len('spectral'):]\n        else:\n            subnorm_type = norm_type\n\n        if subnorm_type == 'none' or len(subnorm_type) == 0:\n            return layer\n\n        # remove bias in the previous layer, which is meaningless\n        # since it has no effect after normalization\n        if getattr(layer, 'bias', None) is not None:\n            delattr(layer, 'bias')\n            layer.register_parameter('bias', None)\n\n        if subnorm_type == 'batch':\n            norm_layer = nn.BatchNorm2d(get_out_channel(layer), affine=True)\n        elif subnorm_type == 'sync_batch':\n            norm_layer = nn.SyncBatchNorm(get_out_channel(layer), affine=True)\n        elif subnorm_type == 'instance':\n            norm_layer = nn.InstanceNorm2d(get_out_channel(layer), affine=False)\n        elif subnorm_type == 'instanceaffine':\n            norm_layer = nn.InstanceNorm2d(get_out_channel(layer), affine=True)\n        else:\n            raise ValueError('normalization layer %s is not recognized' % subnorm_type)\n\n        return nn.Sequential(layer, norm_layer)\n\n    return add_norm_layer\n\n# 5 means nothing here just for initiliazation lol\nnorm_layer = get_nonspade_norm_layer(5)","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:22:34.800295Z","iopub.execute_input":"2024-04-04T00:22:34.802578Z","iopub.status.idle":"2024-04-04T00:22:34.822363Z","shell.execute_reply.started":"2024-04-04T00:22:34.802536Z","shell.execute_reply":"2024-04-04T00:22:34.820997Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class FeatureExtractionModel(nn.Module):\n    def __init__(self, param_shapes,norm_layer = norm_layer,ds_repititions = 4,width = 64,num_out = 1000):\n        super(FeatureExtractionModel, self).__init__()\n        \n        # Load the CLIP model\n        self.clip, _ = clip.load(\"ViT-B/32\", device=\"cpu\")\n        model = []\n        \n        #the input channels is 2 because clip embedding goes from (bs,512) --> (bs,2,16,16)\n        model += [norm_layer(nn.Conv2d(2, width, 3, stride=1, padding=1)),\n                  nn.ReLU(inplace=True)]\n        \n        # this is a learned pointwise convolution repeated 4 times, helps in learning features         \n        for i in range(ds_repititions):\n            model += [norm_layer(nn.Conv2d(width, width, 3, stride=1, padding=1)),\n          nn.ReLU(inplace=True)]\n        \n        # num outputs is supposed to be the number of MLP parameters that we want to estimate          \n        model += [nn.Conv2d(width, num_out, 1)]\n        \n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        # Extract features using the CLIP model\n        x = self.clip.encode_image(x)\n        x = x.view(-1, 2, 16, 16)\n        \n        x = self.model(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:22:34.830255Z","iopub.execute_input":"2024-04-04T00:22:34.832183Z","iopub.status.idle":"2024-04-04T00:22:34.846771Z","shell.execute_reply.started":"2024-04-04T00:22:34.832131Z","shell.execute_reply":"2024-04-04T00:22:34.845726Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Pix2PixDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.image_paths = sorted([os.path.join(data_dir, file_name) for file_name in os.listdir(data_dir) if file_name.endswith('.jpg') or file_name.endswith('.png')])\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        image = Image.open(image_path)\n        width, height = image.size\n\n        # Split the image vertically into two halves\n        left_half = image.crop((0, 0, width // 2, height)) \n        right_half = image.crop((width // 2, 0, width, height))\n\n        # Apply transformations if needed\n        if self.transform:\n            left_half = self.transform(left_half)\n            right_half = self.transform(right_half)\n\n        # Normalize the full-resolution source and target images\n        source_fullres = left_half / 255.0\n        target_highres = right_half / 255.0\n        \n                # Create the x, y coordinates\n        x = torch.linspace(-1, 1, left_half.size(1))\n        y = torch.linspace(-1, 1, left_half.size(2))\n        xx, yy = torch.meshgrid(x, y)\n\n        # Concatenate the image and coordinates\n        source_fullres = torch.cat([left_half, xx.unsqueeze(0), yy.unsqueeze(0)], dim=0)\n#         .view(-1, 5)\n#         target_highres = target_highres.view(-1,3)\n        \n        # Resize the source image to 224x224 and normalize for ResNet input\n        source_lowres = transforms.Resize((224, 224))(left_half)\n        source_lowres = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(source_lowres)\n\n        return source_lowres, source_fullres, target_highres","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:30:29.360924Z","iopub.execute_input":"2024-04-04T00:30:29.361673Z","iopub.status.idle":"2024-04-04T00:30:29.377579Z","shell.execute_reply.started":"2024-04-04T00:30:29.361621Z","shell.execute_reply":"2024-04-04T00:30:29.376317Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Define transformations if needed\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    # Add other transformations if needed\n])\nbatch_size = 64\n\n# Create the dataset\ndataset = Pix2PixDataset(data_dir=\"/kaggle/input/pix2pix-dataset/cityscapes/cityscapes/train/\", transform=transform)\n\n# Create a data loader\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Training loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:30:29.880097Z","iopub.execute_input":"2024-04-04T00:30:29.881145Z","iopub.status.idle":"2024-04-04T00:30:29.897332Z","shell.execute_reply.started":"2024-04-04T00:30:29.881096Z","shell.execute_reply":"2024-04-04T00:30:29.896466Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"for source_lowres, source_fullres, target_highres in data_loader:\n    break","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:30:30.755925Z","iopub.execute_input":"2024-04-04T00:30:30.756437Z","iopub.status.idle":"2024-04-04T00:30:31.810109Z","shell.execute_reply.started":"2024-04-04T00:30:30.756395Z","shell.execute_reply":"2024-04-04T00:30:31.808821Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"source_lowres.shape,source_fullres.shape,target_highres.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:30:31.811939Z","iopub.execute_input":"2024-04-04T00:30:31.812300Z","iopub.status.idle":"2024-04-04T00:30:31.819499Z","shell.execute_reply.started":"2024-04-04T00:30:31.812271Z","shell.execute_reply":"2024-04-04T00:30:31.818542Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(torch.Size([64, 3, 224, 224]),\n torch.Size([64, 5, 256, 256]),\n torch.Size([64, 3, 256, 256]))"},"metadata":{}}]},{"cell_type":"code","source":"k = 16\nbs = 64\nnci = source_fullres.shape[1]\nprint(source_fullres.shape)\n# bs, 5 rgbxy, h//k=h_lr, w//k=w_lr, k, k\ntiles = source_fullres.unfold(2, k, k).unfold(3, k, k)\nprint(tiles.shape)\n\nh_lr = source_fullres.shape[2] // k\nw_lr = source_fullres.shape[3] // k\n\ntiles = tiles.permute(0, 2, 3, 4, 5, 1).contiguous().view(\n    bs, h_lr, w_lr, int(k * k), nci)\nout = tiles","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:35:32.242974Z","iopub.execute_input":"2024-04-04T00:35:32.243540Z","iopub.status.idle":"2024-04-04T00:35:32.315068Z","shell.execute_reply.started":"2024-04-04T00:35:32.243503Z","shell.execute_reply":"2024-04-04T00:35:32.313644Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"torch.Size([64, 5, 256, 256])\ntorch.Size([64, 5, 16, 16, 16, 16])\n","output_type":"stream"}]},{"cell_type":"code","source":"out.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:35:41.686415Z","iopub.execute_input":"2024-04-04T00:35:41.686826Z","iopub.status.idle":"2024-04-04T00:35:41.695450Z","shell.execute_reply.started":"2024-04-04T00:35:41.686794Z","shell.execute_reply":"2024-04-04T00:35:41.693794Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"torch.Size([64, 16, 16, 256, 5])"},"metadata":{}}]},{"cell_type":"code","source":"# Usage example\nmlp = MLP()\nparameter_shapes = get_parameter_shapes(mlp)\nouts = sum(p.numel() for p in mlp.parameters())\nfeature_extraction_model = FeatureExtractionModel(parameter_shapes,num_out = outs)\n\ninput_vector = torch.randn(1,3,224,224)\nestimated_parameters = feature_extraction_model(input_vector)","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:23:03.546399Z","iopub.execute_input":"2024-04-04T00:23:03.546876Z","iopub.status.idle":"2024-04-04T00:23:15.538573Z","shell.execute_reply.started":"2024-04-04T00:23:03.546833Z","shell.execute_reply":"2024-04-04T00:23:15.537253Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|███████████████████████████████████████| 338M/338M [00:05<00:00, 59.5MiB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"parameter_shapes.items()","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:24:44.038593Z","iopub.execute_input":"2024-04-04T00:24:44.039046Z","iopub.status.idle":"2024-04-04T00:24:44.045684Z","shell.execute_reply.started":"2024-04-04T00:24:44.039013Z","shell.execute_reply":"2024-04-04T00:24:44.044694Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"dict_items([('input weights', torch.Size([64, 2])), ('input biases', torch.Size([64])), ('model.0 weights', torch.Size([64, 64])), ('model.0 biases', torch.Size([64])), ('model.2 weights', torch.Size([64, 64])), ('model.2 biases', torch.Size([64])), ('model.4 weights', torch.Size([64, 64])), ('model.4 biases', torch.Size([64])), ('model.6 weights', torch.Size([64, 64])), ('model.6 biases', torch.Size([64])), ('model.8 weights', torch.Size([64, 64])), ('model.8 biases', torch.Size([64])), ('model.10 weights', torch.Size([3, 64])), ('model.10 biases', torch.Size([3]))])"},"metadata":{}}]},{"cell_type":"code","source":"source_fullres.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:59:22.399595Z","iopub.execute_input":"2024-04-04T00:59:22.400111Z","iopub.status.idle":"2024-04-04T00:59:22.408927Z","shell.execute_reply.started":"2024-04-04T00:59:22.400071Z","shell.execute_reply":"2024-04-04T00:59:22.407584Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"torch.Size([64, 5, 256, 256])"},"metadata":{}}]},{"cell_type":"code","source":"def apply_mlp(source_fullres, lr_params, parameter_shapes, k=16):\n    \"\"\"\n    Apply the MLP to the lowres tiles of the input image.\n    \n    Args:\n        source_fullres (torch.Tensor): The full-resolution input image.\n        lr_params (torch.Tensor): The flattened MLP parameters.\n        parameter_shapes (dict): A dictionary containing the shapes of the MLP parameters.\n        k (int): The tile size.\n    \n    Returns:\n        torch.Tensor: The output of the MLP, with the same shape as the original image.\n    \"\"\"\n    bs = source_fullres.size(0)\n    h, w = source_fullres.size(2), source_fullres.size(3)\n    \n    # Compute the lowres height and width\n    h_lr = h // k\n    w_lr = w // k\n    \n    # Unfold the input image into tiles\n    tiles = source_fullres.unfold(2, k, k).unfold(3, k, k)\n    tiles = tiles.permute(0, 2, 3, 4, 5, 1).contiguous().view(bs, h_lr, w_lr, int(k * k), tiles.size(1))\n    \n    # Apply the MLP to the tiles\n    out = tiles\n    layer_idx = 0\n    while f'model.{layer_idx} weights' in parameter_shapes:\n        print(layer_idx)\n        w_shape = parameter_shapes[f'model.{layer_idx} weights']\n        b_shape = parameter_shapes[f'model.{layer_idx} biases']\n#         print(w_shape)\n        nci, nco = w_shape[1], w_shape[0]\n#         print(lr_params)\n        w_ = lr_params[:, :np.prod(w_shape)]\n        b_ = lr_params[:, np.prod(w_shape):np.prod(w_shape) + np.prod(b_shape)]\n        \n        w_ = w_.permute(0, 2, 3, 1).view(bs, h_lr, w_lr, nci, nco)\n        b_ = b_.permute(0, 2, 3, 1).view(bs, h_lr, w_lr, 1, nco)\n        out = torch.matmul(out, w_) + b_\n        \n        if layer_idx < len(parameter_shapes) // 2 - 1:\n            out = torch.nn.functional.leaky_relu(out, 0.01, inplace=True)\n        else:\n            out = torch.nn.functional.tanh(out)\n        \n        layer_idx += 1\n    \n    # Reorder the tiles and reshape the output\n    out = out.view(bs, h_lr, w_lr, k, k, nco).permute(0, 5, 1, 3, 2, 4)\n    out = out.contiguous().view(bs, nco, h, w)\n    \n    return out","metadata":{"execution":{"iopub.status.busy":"2024-04-04T01:05:29.424154Z","iopub.execute_input":"2024-04-04T01:05:29.424565Z","iopub.status.idle":"2024-04-04T01:05:29.444155Z","shell.execute_reply.started":"2024-04-04T01:05:29.424536Z","shell.execute_reply":"2024-04-04T01:05:29.443052Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"21187/64**2","metadata":{"execution":{"iopub.status.busy":"2024-04-04T01:12:51.951265Z","iopub.execute_input":"2024-04-04T01:12:51.951804Z","iopub.status.idle":"2024-04-04T01:12:51.959589Z","shell.execute_reply.started":"2024-04-04T01:12:51.951768Z","shell.execute_reply":"2024-04-04T01:12:51.958201Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"5.172607421875"},"metadata":{}}]},{"cell_type":"code","source":"def apply_mlp(source_fullres, lr_params, parameter_shapes, k=16):\n    \"\"\"\n    Apply the MLP to the lowres tiles of the input image.\n    \n    Args:\n        source_fullres (torch.Tensor): The full-resolution input image.\n        lr_params (torch.Tensor): The flattened MLP parameters.\n        parameter_shapes (dict): A dictionary containing the shapes of the MLP parameters.\n        k (int): The tile size.\n    \n    Returns:\n        torch.Tensor: The output of the MLP, with the same shape as the original image.\n    \"\"\"\n    bs = source_fullres.size(0)\n    h, w = source_fullres.size(2), source_fullres.size(3)\n    \n    # Compute the lowres height and width\n    h_lr = h // k\n    w_lr = w // k\n    \n    # Unfold the input image into tiles\n    tiles = source_fullres.unfold(2, k, k).unfold(3, k, k)\n    tiles = tiles.permute(0, 2, 3, 4, 5, 1).contiguous().view(bs, h_lr, w_lr, int(k * k), tiles.size(1))\n    \n    # Apply the MLP to the tiles\n    out = tiles\n    layer_idx = 0\n    start_idx = 0\n    while f'model.{layer_idx} weights' in parameter_shapes:\n        w_shape = parameter_shapes[f'model.{layer_idx} weights']\n        b_shape = parameter_shapes[f'model.{layer_idx} biases']\n        \n        nci, nco = w_shape[1], w_shape[0]\n        \n        w_size = np.prod(w_shape)\n        b_size = np.prod(b_shape)\n        \n        w_ = lr_params[start_idx:start_idx+w_size]\n        print(w_.shape)\n        w_ = lr_params[start_idx:start_idx+w_size].view(w_shape)\n        b_ = lr_params[start_idx+w_size:start_idx+w_size+b_size].view(b_shape)\n        \n        w_ = w_.view(bs, h_lr, w_lr, nci, nco)\n        b_ = b_.view(bs, h_lr, w_lr, 1, nco)\n        out = torch.matmul(out, w_) + b_\n        \n        if layer_idx < len(parameter_shapes) // 2 - 1:\n            out = torch.nn.functional.leaky_relu(out, 0.01, inplace=True)\n        else:\n            out = torch.nn.functional.tanh(out)\n        \n        start_idx += w_size + b_size\n        layer_idx += 1\n    \n    # Reorder the tiles and reshape the output\n    out = out.view(bs, h_lr, w_lr, k, k, nco).permute(0, 5, 1, 3, 2, 4)\n    out = out.contiguous().view(bs, nco, h, w)\n    \n    return out","metadata":{"execution":{"iopub.status.busy":"2024-04-04T01:09:49.919622Z","iopub.execute_input":"2024-04-04T01:09:49.920109Z","iopub.status.idle":"2024-04-04T01:09:49.933107Z","shell.execute_reply.started":"2024-04-04T01:09:49.920076Z","shell.execute_reply":"2024-04-04T01:09:49.931920Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"(16**2)*(64**2)","metadata":{"execution":{"iopub.status.busy":"2024-04-04T01:14:28.448187Z","iopub.execute_input":"2024-04-04T01:14:28.448627Z","iopub.status.idle":"2024-04-04T01:14:28.456837Z","shell.execute_reply.started":"2024-04-04T01:14:28.448597Z","shell.execute_reply":"2024-04-04T01:14:28.455320Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"1048576"},"metadata":{}}]},{"cell_type":"code","source":"np.prod(torch.Size([1, 21187, 16, 16]))/1048576","metadata":{"execution":{"iopub.status.busy":"2024-04-04T01:15:16.085337Z","iopub.execute_input":"2024-04-04T01:15:16.085781Z","iopub.status.idle":"2024-04-04T01:15:16.094502Z","shell.execute_reply.started":"2024-04-04T01:15:16.085750Z","shell.execute_reply":"2024-04-04T01:15:16.093199Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"5.172607421875"},"metadata":{}}]},{"cell_type":"code","source":"pred = apply_mlp(source_fullres, estimated_parameters, parameter_shapes, k=16)","metadata":{"execution":{"iopub.status.busy":"2024-04-04T01:09:50.419986Z","iopub.execute_input":"2024-04-04T01:09:50.421318Z","iopub.status.idle":"2024-04-04T01:09:50.542500Z","shell.execute_reply.started":"2024-04-04T01:09:50.421263Z","shell.execute_reply":"2024-04-04T01:09:50.540455Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"torch.Size([1, 21187, 16, 16])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mapply_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_fullres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimated_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[66], line 40\u001b[0m, in \u001b[0;36mapply_mlp\u001b[0;34m(source_fullres, lr_params, parameter_shapes, k)\u001b[0m\n\u001b[1;32m     38\u001b[0m w_ \u001b[38;5;241m=\u001b[39m lr_params[start_idx:start_idx\u001b[38;5;241m+\u001b[39mw_size]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(w_\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 40\u001b[0m w_ \u001b[38;5;241m=\u001b[39m \u001b[43mlr_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mw_size\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m b_ \u001b[38;5;241m=\u001b[39m lr_params[start_idx\u001b[38;5;241m+\u001b[39mw_size:start_idx\u001b[38;5;241m+\u001b[39mw_size\u001b[38;5;241m+\u001b[39mb_size]\u001b[38;5;241m.\u001b[39mview(b_shape)\n\u001b[1;32m     43\u001b[0m w_ \u001b[38;5;241m=\u001b[39m w_\u001b[38;5;241m.\u001b[39mview(bs, h_lr, w_lr, nci, nco)\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[64, 64]' is invalid for input of size 5423872"],"ename":"RuntimeError","evalue":"shape '[64, 64]' is invalid for input of size 5423872","output_type":"error"}]},{"cell_type":"code","source":"for name,shape in parameter_shapes.items():\n    print(name)\n    print(shape)\n    print(np.prod(shape))","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:25:05.603434Z","iopub.execute_input":"2024-04-04T00:25:05.603955Z","iopub.status.idle":"2024-04-04T00:25:05.611869Z","shell.execute_reply.started":"2024-04-04T00:25:05.603917Z","shell.execute_reply":"2024-04-04T00:25:05.610339Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"input weights\ntorch.Size([64, 2])\n128\ninput biases\ntorch.Size([64])\n64\nmodel.0 weights\ntorch.Size([64, 64])\n4096\nmodel.0 biases\ntorch.Size([64])\n64\nmodel.2 weights\ntorch.Size([64, 64])\n4096\nmodel.2 biases\ntorch.Size([64])\n64\nmodel.4 weights\ntorch.Size([64, 64])\n4096\nmodel.4 biases\ntorch.Size([64])\n64\nmodel.6 weights\ntorch.Size([64, 64])\n4096\nmodel.6 biases\ntorch.Size([64])\n64\nmodel.8 weights\ntorch.Size([64, 64])\n4096\nmodel.8 biases\ntorch.Size([64])\n64\nmodel.10 weights\ntorch.Size([3, 64])\n192\nmodel.10 biases\ntorch.Size([3])\n3\n","output_type":"stream"}]},{"cell_type":"code","source":"estimated_parameters.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:22:36.941214Z","iopub.status.idle":"2024-04-04T00:22:36.941929Z","shell.execute_reply.started":"2024-04-04T00:22:36.941665Z","shell.execute_reply":"2024-04-04T00:22:36.941688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have already defined the feature_extraction_model\nfor param in feature_extraction_model.clip.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:22:36.943572Z","iopub.status.idle":"2024-04-04T00:22:36.943980Z","shell.execute_reply.started":"2024-04-04T00:22:36.943801Z","shell.execute_reply":"2024-04-04T00:22:36.943817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_mlp(source_fullres, lr_params, k=16, num_layers=None, channels=None):\n    \"\"\"\n    Apply the MLP to the lowres tiles of the input image.\n    \n    Args:\n        source_fullres (torch.Tensor): The full-resolution input image.\n        lr_params (torch.Tensor): The flattened MLP parameters.\n        k (int): The tile size.\n        num_layers (int): The number of layers in the MLP.\n        channels (list): The number of input and output channels for each layer.\n    \n    Returns:\n        torch.Tensor: The output of the MLP, with the same shape as the original image.\n    \"\"\"\n    bs = source_fullres.size(0)\n    nci = source_fullres.size(1)\n    \n    # Compute the lowres height and width\n    h_lr = source_fullres.size(2) // k\n    w_lr = source_fullres.size(3) // k\n    \n    # Unfold the input image into tiles\n    tiles = source_fullres.unfold(2, k, k).unfold(3, k, k)\n    tiles = tiles.permute(0, 2, 3, 4, 5, 1).contiguous().view(bs, h_lr, w_lr, int(k * k), nci)\n    \n    # Apply the MLP to the tiles\n    out = tiles\n    for idx, nco in enumerate(channels[:-1]):\n        nci = channels[idx]\n        bstart, bstop = _get_bias_indices(idx, num_layers)\n        wstart, wstop = _get_weight_indices(idx, num_layers)\n        \n        w_ = lr_params[:, wstart:wstop]\n        b_ = lr_params[:, bstart:bstop]\n        \n        w_ = w_.permute(0, 2, 3, 1).view(bs, h_lr, w_lr, nci, nco)\n        b_ = b_.permute(0, 2, 3, 1).view(bs, h_lr, w_lr, 1, nco)\n        out = torch.matmul(out, w_) + b_\n        \n        if idx < num_layers - 1:\n            out = torch.nn.functional.leaky_relu(out, 0.01, inplace=True)\n        else:\n            out = torch.nn.functional.tanh(out)\n    \n    # Reorder the tiles and reshape the output\n    out = out.view(bs, h_lr, w_lr, k, k, channels[-1]).permute(0, 5, 1, 3, 2, 4)\n    out = out.contiguous().view(bs, channels[-1], source_fullres.size(2), source_fullres.size(3))\n    \n    return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.MSELoss()\n# criterion = losses.ssim(window_size=11, reduction='mean', max_val=1.0)\noptimizer = torch.optim.Adam(feature_extraction_model.parameters(), lr=0.001)\n\nfor batch in data_loader:\n    source_lowres, source_fullres, target_highres = batch\n\n    # Extract the unique parameters for each input in the batch\n    outs = feature_extraction_model(source_lowres)\n    for name,shape in outs.items():\n        if \n    break","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:22:36.945716Z","iopub.status.idle":"2024-04-04T00:22:36.946089Z","shell.execute_reply.started":"2024-04-04T00:22:36.945916Z","shell.execute_reply":"2024-04-04T00:22:36.945931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outs","metadata":{"execution":{"iopub.status.busy":"2024-04-04T00:22:36.947481Z","iopub.status.idle":"2024-04-04T00:22:36.947875Z","shell.execute_reply.started":"2024-04-04T00:22:36.947697Z","shell.execute_reply":"2024-04-04T00:22:36.947713Z"},"trusted":true},"execution_count":null,"outputs":[]}]}